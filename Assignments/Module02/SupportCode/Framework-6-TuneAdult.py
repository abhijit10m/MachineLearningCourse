kOutputDirectory = "/Users/bhatnaa/Documents/uw/csep546/module1/MachineLearningCourse/MachineLearningCourse/visualize/Module2/Assignment6/"

import MachineLearningCourse.MLUtilities.Evaluations.EvaluateBinaryClassification as EvaluateBinaryClassification
import MachineLearningCourse.MLUtilities.Evaluations.ErrorBounds as ErrorBounds

import MachineLearningCourse.MLUtilities.Learners.DecisionTreeWeighted as DecisionTreeWeighted
import MachineLearningCourse.MLUtilities.Learners.AdaBoost as AdaBoost
import MachineLearningCourse.MLUtilities.Learners.LogisticRegression as LogisticRegression
import MachineLearningCourse.MLUtilities.Visualizations.Charting as Charting

import MachineLearningCourse.MLProjectSupport.Adult.AdultDataset as AdultDataset
import MachineLearningCourse.MLUtilities.Data.Sample as Sample
import MachineLearningCourse.Assignments.Module02.SupportCode.AdultFeaturize as AdultFeaturize

import numpy as np
import multiprocessing.dummy as mp
import logging
from joblib import Parallel, delayed


sweeps = dict()

sweeps['maxDepth'] = [1,5,10,12,15]
# sweeps['rounds'] = [1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]
sweeps['rounds'] = [1,5,10,15,20,25,30,35,40,45,50]

(xRaw, yRaw) = AdultDataset.LoadRawData()

(xTrainRaw, yTrain, xValidateRaw, yValidate, xTestRaw, yTest) = Sample.TrainValidateTestSplit(xRaw, yRaw, percentValidate=.1, percentTest=.1)

xValidateRaw = np.array(xValidateRaw)
yValidate = np.array(yValidate)
print("Train is %d samples, %.4f percent >50K." % (len(yTrain), 100.0 * sum(yTrain)/len(yTrain)))
print("Validate is %d samples, %.4f percent >50K." % (len(yValidate), 100.0 * sum(yValidate)/len(yValidate)))
print("Test is %d samples %.4f percent >50K." % (len(yTest), 100.0 * sum(yTest)/len(yTest)))

featurizer = AdultFeaturize.AdultFeaturize()
featurizer.CreateFeatureSet(xTrainRaw, yTrain, useCategoricalFeatures = True, useNumericFeatures = True, normalize=False)

xTrain      = np.asarray(featurizer.Featurize(xTrainRaw))
xValidate   = np.asarray(featurizer.Featurize(xValidateRaw))
xTest   = np.asarray(featurizer.Featurize(xTestRaw))


for i in range(featurizer.GetFeatureCount()):
    print("%d - %s" % (i, featurizer.GetFeatureInfo(i)))

for i in range(10):
    print("%d - " % (yTrain[i]), xTrain[i])

# A helper function for calculating FN rate and FP rate across a range of thresholds
def TabulateModelPerformanceForROC(model, xValidate, yValidate):
    global f
    pointsToEvaluate = 100
    thresholds = [ x / float(pointsToEvaluate) for x in range(pointsToEvaluate + 1)]
    FPR_dict = dict()
    FNR_dict = dict()

    FPRs = []
    FNRs = []


    def f(threshold):
        try:
            yPredict = model.predict(xValidate, classificationThreshold=threshold)
            FPR = EvaluateBinaryClassification.FalsePositiveRate(yValidate, yPredict)
            FNR = EvaluateBinaryClassification.FalseNegativeRate(yValidate, yPredict)
            logging.debug("threshold : %s, FPR: %s, FNR: %s", threshold, FPR, FNR)
        except NotImplementedError:
            raise UserWarning("The 'model' parameter must have a 'predict' method that supports using a 'classificationThreshold' parameter with range [ 0 - 1.0 ] to create classifications.")

        FPR_dict[threshold] = FPR
        FNR_dict[threshold] = FNR
        return (threshold, FPR, FNR)


    try:
        with mp.Pool() as p:
            p.map(f, thresholds)
        p.close()
        p.join()
        # for t in thresholds:
        #     f(t)

        for threshold in thresholds:
            FPRs.append(FPR_dict[threshold])
            FNRs.append(FNR_dict[threshold])
    except NotImplementedError:
        raise UserWarning("The 'model' parameter must have a 'predict' method that supports using a 'classificationThreshold' parameter with range [ 0 - 1.0 ] to create classifications.")

    return (FPRs, FNRs, thresholds)

def plotValidationVsTrainingSetAccuracy(models, sweepName):
    yValues = []
    errorBars = []
    seriesName = ["Training set accuracy","Validation set accuracy"]
    xValues = []
    sweeps[sweepName]

    yValues_train = []
    errorBars_train = []
    yValues_validate = []
    errorBars_validate = []
    min_lowerBound = 1.0
    counter = 0
    for (sweep, model) in models:
        
        # Training Set Accuracy
        yPredict = model.predict(xTrain)
        EvaluateBinaryClassification.ExecuteAll(yTrain, yPredict)
        accuracy = EvaluateBinaryClassification.Accuracy(yTrain, yPredict)
        (lowerBound, upperBound) = ErrorBounds.GetAccuracyBounds(accuracy, len(xTrain), .5)
        yValues_train.append(accuracy)
        errorBars_train.append(accuracy - lowerBound)

        # Validation Set Accuracy
        yPredict = model.predict(xValidate)
        EvaluateBinaryClassification.ExecuteAll(yValidate, yPredict)
        accuracy = EvaluateBinaryClassification.Accuracy(yValidate, yPredict)
        (lowerBound, upperBound) = ErrorBounds.GetAccuracyBounds(accuracy, len(xValidate), .5)
        yValues_validate.append(accuracy)
        errorBars_validate.append(accuracy - lowerBound)
        if lowerBound < min_lowerBound:
            min_lowerBound = lowerBound

        xValues.append(sweeps[sweepName][counter])
        counter += 1
        print("yValues_train %", yValues_train)
        print("yValues_validate %", yValues_train)
        print("errorBars_train %", errorBars_train)
        print("errorBars_validate %", errorBars_validate)
        print("seriesName %", seriesName)
        print("xValues %", xValues)
        Charting.PlotSeriesWithErrorBars([yValues_train, yValues_validate], [errorBars_train, errorBars_validate], seriesName, xValues, chartTitle="Validation Set vs Training Set accuracy", xAxisTitle=sweepName, yAxisTitle="Accuracy", yBotLimit=min_lowerBound - 0.01, outputDirectory=kOutputDirectory, fileName="TestSetAccuracyVsNumberOfRounds_{}".format(sweepName))

# ## rounds hyperparamter in BoostedTrees
# models = []
# maxDepth = 12
# adaBoost = AdaBoost.AdaBoost(sweeps['rounds'][-1], DecisionTreeWeighted.DecisionTreeWeighted, maxDepth=maxDepth)
# adaBoostModel = adaBoost.adaBoost(xTrain.tolist(), yTrain)


# for round in sweeps['rounds']:
#     models.append((round, adaBoostModel.getModelWithRounds(round)))

# plotValidationVsTrainingSetAccuracy(models, "rounds")

# maxDepth for decision trees
# models = []

# def evaluateBoostedTree(maxDepth):
#     rounds = 10
#     adaBoost = AdaBoost.AdaBoost(rounds, DecisionTreeWeighted.DecisionTreeWeighted, maxDepth=maxDepth)
#     adaBoostModel = adaBoost.adaBoost(xTrain.tolist(), yTrain)
#     return (maxDepth, adaBoostModel)

# models = Parallel(n_jobs=6)(delayed(evaluateBoostedTree)(maxDepth) for maxDepth in sweeps['maxDepth'])

# models = sorted(models, key = lambda x: x[0])
# plotValidationVsTrainingSetAccuracy(models, "maxDepth")



bestMaxDepth=12
bestRounds=10
bestStepSize=0.001
bestConvergence=0.0001
seriesLabels = ["Train", "Test"]


### Boosted Tree 


seriesFPRs = []
seriesFNRs = []

adaBoost = AdaBoost.AdaBoost(bestRounds, DecisionTreeWeighted.DecisionTreeWeighted, maxDepth=bestMaxDepth)
adaBoostModel = adaBoost.adaBoost(xTrain.tolist(), yTrain)

print("Training complete")

(adaBoostModelFPRs, adaBoostModelFNRs, thresholdsadaBoostModel) = TabulateModelPerformanceForROC(adaBoostModel, xTrain, yTrain)
seriesFPRs.append(adaBoostModelFPRs)
seriesFNRs.append(adaBoostModelFNRs)

print("Unweighted Boosted Tree Train Rate {}".format(list(zip(thresholdsadaBoostModel, adaBoostModelFPRs, adaBoostModelFNRs))))

(adaBoostModelFPRs, adaBoostModelFNRs, thresholdsadaBoostModel) = TabulateModelPerformanceForROC(adaBoostModel, xTest, yTest)
seriesFPRs.append(adaBoostModelFPRs)
seriesFNRs.append(adaBoostModelFNRs)

print("Unweighted Boosted Tree Test Rate {}".format(list(zip(thresholdsadaBoostModel, adaBoostModelFPRs, adaBoostModelFNRs))))


Charting.PlotROCs(seriesFPRs, seriesFNRs, seriesLabels, useLines=True, chartTitle="ROC BoostedTrees Train vs Test set", xAxisTitle="False Negative Rate", yAxisTitle="False Positive Rate", outputDirectory=kOutputDirectory, fileName="Plot-Adult-BestModel-2")




# ### Unweighted Decision Tree 
# seriesFPRs = []
# seriesFNRs = []

# modelWeighted = DecisionTreeWeighted.DecisionTreeWeighted()
# wTrain = [ 1.0 for example in xTrain]
# modelWeighted.fit(xTrain.tolist(), yTrain, wTrain, maxDepth=bestMaxDepth, verbose=True)
# modelWeighted.visualize()

# (modelWeightedFPRs, modelWeightedFNRs, thresholdsWeighted) = TabulateModelPerformanceForROC(modelWeighted, xTrain, yTrain)
# seriesFPRs.append(modelWeightedFPRs)
# seriesFNRs.append(modelWeightedFNRs)

# print("Unweighted Decision Tree Train Rate {}".format(list(zip(thresholdsWeighted, modelWeightedFPRs, modelWeightedFNRs))))


# (modelWeightedFPRs, modelWeightedFNRs, thresholdsWeighted) = TabulateModelPerformanceForROC(modelWeighted, xTest, yTest)
# seriesFPRs.append(modelWeightedFPRs)
# seriesFNRs.append(modelWeightedFNRs)

# print("Unweighted Decision Tree Test Rate {}".format(list(zip(thresholdsWeighted, modelWeightedFPRs, modelWeightedFNRs))))

# Charting.PlotROCs(seriesFPRs, seriesFNRs, seriesLabels, useLines=True, chartTitle="ROC Decision Trees Train vs Test set", xAxisTitle="False Negative Rate", yAxisTitle="False Positive Rate", outputDirectory=kOutputDirectory, fileName="Plot-Adult-BestModel-1")





# # ### Logistic Regression
 
# seriesFPRs = []
# seriesFNRs = []

# featurizer = AdultFeaturize.AdultFeaturize()
# featurizer.CreateFeatureSet(xTrainRaw, yTrain, useCategoricalFeatures = True, useNumericFeatures = True, normalize=True)

# xTrain      = np.asarray(featurizer.Featurize(xTrainRaw))
# xValidate   = np.asarray(featurizer.Featurize(xValidateRaw))
# xTest       = np.asarray(featurizer.Featurize(xTestRaw))

# model = LogisticRegression.LogisticRegression()
# model.fit(xTrain, np.asarray(yTrain), convergence=bestConvergence, stepSize=bestStepSize, verbose=True)


# (lgFPRs, lgModelFNRs, lgthresholds) = TabulateModelPerformanceForROC(model, xTrain, yTrain)
# seriesFPRs.append(lgFPRs)
# seriesFNRs.append(lgModelFNRs)

# print("LogisticRegression Train Rate {}".format(list(zip(lgthresholds, seriesFPRs, seriesFNRs))))

# (lgFPRs, lgModelFNRs, lgthresholds) = TabulateModelPerformanceForROC(model, xTest, yTest)
# seriesFPRs.append(lgFPRs)
# seriesFNRs.append(lgModelFNRs)

# print("LogisticRegression Test Rate {}".format(list(zip(lgthresholds, seriesFPRs, seriesFNRs))))


# Charting.PlotROCs(seriesFPRs, seriesFNRs, seriesLabels, useLines=True, chartTitle="ROC Logistic Regression Train vs Test set", xAxisTitle="False Negative Rate", yAxisTitle="False Positive Rate", outputDirectory=kOutputDirectory, fileName="Plot-Adult-BestModel-3")





# Unweighted Decision Tree Train Rate [(0.0, 1.0, 0.0), (0.01, 0.6761576971214017, 0.0004939897908776552), (0.02, 0.5801522736754277, 0.0031286020088918164), (0.03, 0.49942636629119735, 0.008233163181294254), (0.04, 0.40962661660408844, 0.017124979417092047), (0.05, 0.39883187317480184, 0.017124979417092047), (0.06, 0.38923654568210264, 0.017783632471595586), (0.07, 0.38084063412599084, 0.018771612053350897), (0.08, 0.3562265331664581, 0.023382183434875678), (0.09, 0.35518356278681684, 0.023382183434875678), (0.1, 0.35226324572382145, 0.02371150996212745), (0.11, 0.3220692532332082, 0.033920632306932326), (0.12, 0.31899249061326657, 0.03441462209780998), (0.13, 0.30851063829787234, 0.038201877161205336), (0.14, 0.30720692532332083, 0.03869586695208299), (0.15, 0.29792448894451395, 0.04231845875185246), (0.16, 0.2622549019607843, 0.06224271365058456), (0.17, 0.25484981226533165, 0.06602996871397991), (0.18, 0.24932206925323322, 0.0694878972501235), (0.19, 0.2453066332916145, 0.07195784620451177), (0.2, 0.22152690863579474, 0.09023546846698502), (0.21, 0.20666458072590738, 0.10077391733904166), (0.22, 0.2031706299541093, 0.10357319282068171), (0.23, 0.1947747183979975, 0.11065371315659477), (0.24, 0.1947747183979975, 0.11065371315659477), (0.25, 0.1947747183979975, 0.11065371315659477), (0.26, 0.19070713391739674, 0.11230034579285361), (0.27, 0.16066958698372966, 0.1467149678906636), (0.28, 0.11795994993742177, 0.1985838959328174), (0.29, 0.11775135586149353, 0.19874855919644327), (0.3, 0.11775135586149353, 0.19874855919644327), (0.31, 0.11274509803921569, 0.20566441626873044), (0.32, 0.11274509803921569, 0.20566441626873044), (0.33, 0.11003337505214852, 0.20961633459575169), (0.34, 0.10768669169795578, 0.21093364070475876), (0.35, 0.10768669169795578, 0.21093364070475876), (0.36, 0.10768669169795578, 0.21093364070475876), (0.37, 0.10737380058406341, 0.21142763049563643), (0.38, 0.08010012515644556, 0.2634612218014161), (0.39, 0.07937004589069671, 0.26477852791042317), (0.4, 0.07937004589069671, 0.26477852791042317), (0.41, 0.0784313725490196, 0.2662604972830562), (0.42, 0.0784313725490196, 0.2662604972830562), (0.43, 0.0784313725490196, 0.2662604972830562), (0.44, 0.0784313725490196, 0.2662604972830562), (0.45, 0.07665832290362953, 0.27070640540095503), (0.46, 0.07665832290362953, 0.27070640540095503), (0.47, 0.07665832290362953, 0.27070640540095503), (0.48, 0.07665832290362953, 0.27070640540095503), (0.49, 0.07206925323320817, 0.2843734562819035), (0.5, 0.07206925323320817, 0.2843734562819035), (0.51, 0.07034835210680017, 0.2898073439815577), (0.52, 0.07034835210680017, 0.2898073439815577), (0.53, 0.033322903629536924, 0.41972665898238104), (0.54, 0.033322903629536924, 0.41972665898238104), (0.55, 0.033322903629536924, 0.41972665898238104), (0.56, 0.03316645807259074, 0.42038531203688456), (0.57, 0.03316645807259074, 0.42038531203688456), (0.58, 0.033062161034626614, 0.42087930182776223), (0.59, 0.033062161034626614, 0.42087930182776223), (0.6, 0.03170629954109303, 0.42713650584554586), (0.61, 0.03139340842720067, 0.4291124650090565), (0.62, 0.03139340842720067, 0.4291124650090565), (0.63, 0.01882561535252399, 0.49662440309566935), (0.64, 0.01882561535252399, 0.49662440309566935), (0.65, 0.018617021276595744, 0.49794170920467645), (0.66, 0.018095536086775135, 0.5012349744771941), (0.67, 0.018043387567793074, 0.5096328009221143), (0.68, 0.018043387567793074, 0.5096328009221143), (0.69, 0.018043387567793074, 0.5096328009221143), (0.7, 0.018043387567793074, 0.5096328009221143), (0.71, 0.017939090529828953, 0.5106207805038696), (0.72, 0.017626199415936586, 0.5140787090400132), (0.73, 0.013871506049228202, 0.5460233821834348), (0.74, 0.013871506049228202, 0.5460233821834348), (0.75, 0.003389653733833959, 0.6413634118228223), (0.76, 0.0031810596579057156, 0.6492672484768648), (0.77, 0.0031810596579057156, 0.6492672484768648), (0.78, 0.0031289111389236545, 0.6502552280586201), (0.79, 0.0031289111389236545, 0.6502552280586201), (0.8, 0.0031289111389236545, 0.6502552280586201), (0.81, 0.0031289111389236545, 0.6551951259673967), (0.82, 0.0031289111389236545, 0.6551951259673967), (0.83, 0.0031289111389236545, 0.6551951259673967), (0.84, 0.0031289111389236545, 0.6598056973489215), (0.85, 0.00031289111389236547, 0.7080520335913058), (0.86, 0.00031289111389236547, 0.7113452988638235), (0.87, 0.00031289111389236547, 0.7113452988638235), (0.88, 0.00031289111389236547, 0.7182611559361106), (0.89, 0.00010429703796412182, 0.7266589823810308), (0.9, 5.214851898206091e-05, 0.729293594599045), (0.91, 5.214851898206091e-05, 0.7404906965256052), (0.92, 5.214851898206091e-05, 0.7404906965256052), (0.93, 5.214851898206091e-05, 0.7424666556891157), (0.94, 0.0, 0.7495471760250288), (0.95, 0.0, 0.7495471760250288), (0.96, 0.0, 0.7530051045611724), (0.97, 0.0, 0.7576156759426972), (0.98, 0.0, 0.7780339206323069), (0.99, 0.0, 0.7898896756133706), (1.0, 0.0, 1.0)]
# Unweighted Decision Tree Test Rate [(0.0, 1.0, 0.0), (0.01, 0.6697517879680269, 0.0038510911424903724), (0.02, 0.5734118636937316, 0.010269576379974325), (0.03, 0.49347917543121583, 0.023106546854942234), (0.04, 0.3984013462347497, 0.03080872913992298), (0.05, 0.3904080774084981, 0.037227214377406934), (0.06, 0.3840976020193521, 0.038510911424903725), (0.07, 0.37484223811527134, 0.03979460847240052), (0.08, 0.35128312999579303, 0.0410783055198973), (0.09, 0.35128312999579303, 0.04236200256739409), (0.1, 0.349179638199411, 0.043645699614890884), (0.11, 0.3239377366428271, 0.059050064184852376), (0.12, 0.3214135464871687, 0.06161745827984596), (0.13, 0.3092132940681531, 0.07188703465982028), (0.14, 0.30837189734960035, 0.07188703465982028), (0.15, 0.30122002524190156, 0.07317073170731707), (0.16, 0.2768195204038704, 0.08985879332477535), (0.17, 0.27092974337400083, 0.0962772785622593), (0.18, 0.26672275978123683, 0.10012836970474968), (0.19, 0.2625157761884729, 0.10269576379974327), (0.2, 0.23643247791333613, 0.11681643132220795), (0.21, 0.22381152713504418, 0.12708600770218229), (0.22, 0.22170803533866218, 0.12836970474967907), (0.23, 0.2145561632309634, 0.13350449293966624), (0.24, 0.2145561632309634, 0.13350449293966624), (0.25, 0.2145561632309634, 0.13350449293966624), (0.26, 0.2111905763567522, 0.13607188703465983), (0.27, 0.18384518300378627, 0.1668806161745828), (0.28, 0.139251156920488, 0.22849807445442877), (0.29, 0.1384097602019352, 0.22849807445442877), (0.3, 0.1384097602019352, 0.22849807445442877), (0.31, 0.13378207824989483, 0.2362002567394095), (0.32, 0.13378207824989483, 0.2362002567394095), (0.33, 0.13167858645351282, 0.24133504492939667), (0.34, 0.12957509465713082, 0.24775353016688062), (0.35, 0.12957509465713082, 0.24775353016688062), (0.36, 0.12957509465713082, 0.24775353016688062), (0.37, 0.12873369793857803, 0.24775353016688062), (0.38, 0.10096760622633572, 0.3042362002567394), (0.39, 0.10054690786705932, 0.3055198973042362), (0.4, 0.10054690786705932, 0.3055198973042362), (0.41, 0.10054690786705932, 0.306803594351733), (0.42, 0.10054690786705932, 0.306803594351733), (0.43, 0.10054690786705932, 0.306803594351733), (0.44, 0.10054690786705932, 0.306803594351733), (0.45, 0.09760201935212452, 0.31835686777920413), (0.46, 0.09760201935212452, 0.31835686777920413), (0.47, 0.09760201935212452, 0.31835686777920413), (0.48, 0.09760201935212452, 0.31835686777920413), (0.49, 0.09339503575936053, 0.3337612323491656), (0.5, 0.09339503575936053, 0.3337612323491656), (0.51, 0.09087084560370215, 0.3337612323491656), (0.52, 0.09087084560370215, 0.3337612323491656), (0.53, 0.055952881783761045, 0.4685494223363286), (0.54, 0.055952881783761045, 0.4685494223363286), (0.55, 0.055952881783761045, 0.4685494223363286), (0.56, 0.055952881783761045, 0.4685494223363286), (0.57, 0.055952881783761045, 0.4685494223363286), (0.58, 0.055952881783761045, 0.4685494223363286), (0.59, 0.055952881783761045, 0.4685494223363286), (0.6, 0.054690786705931846, 0.4762516046213094), (0.61, 0.053428691628102654, 0.47753530166880614), (0.62, 0.053428691628102654, 0.47753530166880614), (0.63, 0.034917963819941104, 0.55198973042362), (0.64, 0.034917963819941104, 0.55198973042362), (0.65, 0.0340765671013883, 0.55198973042362), (0.66, 0.032393773664282706, 0.5545571245186136), (0.67, 0.030290281867900715, 0.5571245186136072), (0.68, 0.030290281867900715, 0.5571245186136072), (0.69, 0.030290281867900715, 0.5571245186136072), (0.7, 0.030290281867900715, 0.5571245186136072), (0.71, 0.030290281867900715, 0.558408215661104), (0.72, 0.029869583508624318, 0.5596919127086007), (0.73, 0.026503996634413125, 0.5905006418485238), (0.74, 0.026503996634413125, 0.5905006418485238), (0.75, 0.013041649137568364, 0.6739409499358151), (0.76, 0.011358855700462769, 0.6765083440308087), (0.77, 0.011358855700462769, 0.6765083440308087), (0.78, 0.011358855700462769, 0.6777920410783055), (0.79, 0.011358855700462769, 0.6777920410783055), (0.8, 0.011358855700462769, 0.6777920410783055), (0.81, 0.011358855700462769, 0.6777920410783055), (0.82, 0.011358855700462769, 0.6777920410783055), (0.83, 0.011358855700462769, 0.6777920410783055), (0.84, 0.008413967185527976, 0.6790757381258024), (0.85, 0.005469078670593185, 0.7137355584082157), (0.86, 0.005048380311316786, 0.7163029525032092), (0.87, 0.005048380311316786, 0.7163029525032092), (0.88, 0.004206983592763988, 0.7227214377406932), (0.89, 0.002944888514934792, 0.7252888318356868), (0.9, 0.002524190155658393, 0.7265725288831836), (0.91, 0.0012620950778291964, 0.7342747111681643), (0.92, 0.0012620950778291964, 0.7342747111681643), (0.93, 0.0012620950778291964, 0.7368421052631579), (0.94, 0.00042069835927639884, 0.7445442875481386), (0.95, 0.00042069835927639884, 0.7445442875481386), (0.96, 0.00042069835927639884, 0.7458279845956355), (0.97, 0.00042069835927639884, 0.7509627727856226), (0.98, 0.0, 0.7740693196405648), (0.99, 0.0, 0.7869062901155327), (1.0, 0.0, 1.0)]

# LogisticRegression Train Rate [(0.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998435544430538, 0.9673028785982478, 0.9027951606174385, 0.851063829787234, 0.7829578639966625, 0.7053087192323738, 0.6365769712140176, 0.5738423028785983, 0.5216416353775553, 0.4821130579891531, 0.4505110554860242, 0.41724030037546933, 0.39413850646641635, 0.3739048811013767, 0.3610241969128077, 0.3482999582811848, 0.3335419274092616, 0.28003754693366706, 0.24953066332916146, 0.23138297872340424, 0.20875052148518983, 0.1942532332081769, 0.1584793491864831, 0.12286191072173551, 0.10596579057154777, 0.09115561118064247, 0.0791093032957864, 0.06200458906967042, 0.04750730079265749, 0.03170629954109303, 0.023884021693783897, 0.01939924906132666, 0.01491447642886942, 0.008448060075093867, 0.004380475594493116, 0.003285356695869837, 0.0023988318731748017, 0.0015644555694618273, 0.0011994159365874009, 0.001095118898623279, 0.00020859407592824363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0011526428453811955, 0.003293265272517701, 0.004939897908776552, 0.0110324386629343, 0.021900214062242712, 0.033920632306932326, 0.05532685657829738, 0.06619463197760579, 0.07656841758603655, 0.08842417256710028, 0.10884241725671003, 0.11493495801086777, 0.12629672320105384, 0.13617651901860695, 0.1467149678906636, 0.15807673308084966, 0.22114276304956365, 0.25671002799275483, 0.29491190515396015, 0.3406882924419562, 0.36736374114934955, 0.4114934958010868, 0.48312201547834677, 0.5298863823480982, 0.591635106207805, 0.6527251770130084, 0.7531697678247983, 0.7752346451506669, 0.8142598386300016, 0.8399473077556397, 0.8713979911081837, 0.9063066029968714, 0.9520829902848674, 0.9746418574016137, 0.9830396838465338, 0.9846863164827927, 0.9858389593281739, 0.990120204182447, 0.9939074592458422, 0.9980240408364894, 0.9996706734727482, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])]
# LogisticRegression Test Rate [(0.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998435544430538, 0.9673028785982478, 0.9027951606174385, 0.851063829787234, 0.7829578639966625, 0.7053087192323738, 0.6365769712140176, 0.5738423028785983, 0.5216416353775553, 0.4821130579891531, 0.4505110554860242, 0.41724030037546933, 0.39413850646641635, 0.3739048811013767, 0.3610241969128077, 0.3482999582811848, 0.3335419274092616, 0.28003754693366706, 0.24953066332916146, 0.23138297872340424, 0.20875052148518983, 0.1942532332081769, 0.1584793491864831, 0.12286191072173551, 0.10596579057154777, 0.09115561118064247, 0.0791093032957864, 0.06200458906967042, 0.04750730079265749, 0.03170629954109303, 0.023884021693783897, 0.01939924906132666, 0.01491447642886942, 0.008448060075093867, 0.004380475594493116, 0.003285356695869837, 0.0023988318731748017, 0.0015644555694618273, 0.0011994159365874009, 0.001095118898623279, 0.00020859407592824363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0011526428453811955, 0.003293265272517701, 0.004939897908776552, 0.0110324386629343, 0.021900214062242712, 0.033920632306932326, 0.05532685657829738, 0.06619463197760579, 0.07656841758603655, 0.08842417256710028, 0.10884241725671003, 0.11493495801086777, 0.12629672320105384, 0.13617651901860695, 0.1467149678906636, 0.15807673308084966, 0.22114276304956365, 0.25671002799275483, 0.29491190515396015, 0.3406882924419562, 0.36736374114934955, 0.4114934958010868, 0.48312201547834677, 0.5298863823480982, 0.591635106207805, 0.6527251770130084, 0.7531697678247983, 0.7752346451506669, 0.8142598386300016, 0.8399473077556397, 0.8713979911081837, 0.9063066029968714, 0.9520829902848674, 0.9746418574016137, 0.9830396838465338, 0.9846863164827927, 0.9858389593281739, 0.990120204182447, 0.9939074592458422, 0.9980240408364894, 0.9996706734727482, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]), (0.01, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9991586032814472, 0.9667648296171645, 0.8981909970551115, 0.8435002103491797, 0.7782919646613379, 0.6996213714766513, 0.6217921750105174, 0.5599495161968868, 0.5044173327724022, 0.46992006731173747, 0.4371055952881784, 0.4059739167017249, 0.37904922170803534, 0.363904080774085, 0.3521245267143458, 0.34328986116954147, 0.3310896087505259, 0.278502313840976, 0.2486327303323517, 0.23138409760201936, 0.20992848127892302, 0.19562473706352546, 0.16070677324358434, 0.12747160286074885, 0.11064366848969288, 0.09802271771140092, 0.08077408498106857, 0.06436684896928901, 0.049221708035338665, 0.031131678586453514, 0.02440050483803113, 0.01809002944888515, 0.013883045856121162, 0.00715187210769878, 0.004627681952040387, 0.004206983592763988, 0.0037862852334875894, 0.002524190155658393, 0.002103491796381994, 0.0016827934371055953, 0.0008413967185527977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025673940949935813, 0.006418485237483954, 0.011553273427471117, 0.01668806161745828, 0.026957637997432605, 0.04749679075738126, 0.05263157894736842, 0.057766367137355584, 0.0693196405648267, 0.08729139922978177, 0.0962772785622593, 0.10911424903722722, 0.12323491655969192, 0.12836970474967907, 0.1527599486521181, 0.21052631578947367, 0.25802310654685495, 0.2939666238767651, 0.3363286264441592, 0.37098844672657255, 0.42233632862644416, 0.4942233632862644, 0.5494223363286265, 0.6033376123234917, 0.6521181001283697, 0.7496790757381258, 0.7843388960205392, 0.810012836970475, 0.834403080872914, 0.8613607188703466, 0.889602053915276, 0.9422336328626444, 0.9691912708600771, 0.9768934531450578, 0.9807445442875481, 0.9845956354300385, 0.9884467265725289, 0.9897304236200257, 0.9974326059050064, 0.9987163029525032, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])]

# Unweighted Boosted Tree Train Rate [(0.0, 1.0, 0.0), (0.01, 1.0, 0.0), (0.02, 1.0, 0.0), (0.03, 1.0, 0.0), (0.04, 1.0, 0.0), (0.05, 1.0, 0.0), (0.06, 1.0, 0.0), (0.07, 1.0, 0.0), (0.08, 1.0, 0.0), (0.09, 1.0, 0.0), (0.1, 1.0, 0.0), (0.11, 0.7436378806841886, 0.0), (0.12, 0.7436378806841886, 0.0), (0.13, 0.7436378806841886, 0.0), (0.14, 0.7067167292448895, 0.0), (0.15, 0.6801731330830204, 0.0), (0.16, 0.6343345848977889, 0.0), (0.17, 0.6225490196078431, 0.0), (0.18, 0.5724864413850647, 0.0), (0.19, 0.5361910721735502, 0.0), (0.2, 0.5294117647058824, 0.0), (0.21, 0.5079787234042553, 0.0), (0.22, 0.4906654151022111, 0.0), (0.23, 0.4650604922820192, 0.00016466326362588507), (0.24, 0.4602106800166875, 0.00016466326362588507), (0.25, 0.4326762619941594, 0.00032932652725177014), (0.26, 0.4231852315394243, 0.0008233163181294253), (0.27, 0.3877763871506049, 0.0014819693726329656), (0.28, 0.3777117229870672, 0.0016466326362588506), (0.29, 0.3738005840634126, 0.0016466326362588506), (0.3, 0.3600333750521485, 0.002799275481640046), (0.31, 0.34908218606591573, 0.004610571381524782), (0.32, 0.32540675844806005, 0.007739173390416598), (0.33, 0.31867959949937424, 0.008562489708546023), (0.34, 0.2917709637046308, 0.012843734562819035), (0.35, 0.2796203587818106, 0.01663098962621439), (0.36, 0.2618898623279099, 0.022229540589494483), (0.37, 0.25735294117647056, 0.02404083648937922), (0.38, 0.2437421777221527, 0.028157418080026347), (0.39, 0.2261159783062161, 0.03556726494319117), (0.4, 0.22376929495202336, 0.0363905812613206), (0.41, 0.2005110554860242, 0.05055162193314672), (0.42, 0.1935753024614101, 0.05582084636917504), (0.43, 0.13266583229036297, 0.11295899884735715), (0.44, 0.1307363370880267, 0.1157582743289972), (0.45, 0.1287546933667084, 0.11872221307426313), (0.46, 0.09073842302878599, 0.181952906306603), (0.47, 0.07321652065081352, 0.21966079367693067), (0.48, 0.06409052982895286, 0.24090235468466986), (0.49, 0.0625782227784731, 0.24683023217520172), (0.5, 0.05329578639966625, 0.27037707887370327), (0.51, 0.03144555694618273, 0.3614358636588177), (0.52, 0.027169378389653734, 0.3769142104396509), (0.53, 0.02304964539007092, 0.40079038366540426), (0.54, 0.020233625365039632, 0.42384324057302813), (0.55, 0.015123070504797664, 0.45611724024370165), (0.56, 0.011681268251981644, 0.48493331137823154), (0.57, 0.010794743429286609, 0.48921455623250454), (0.58, 0.008448060075093867, 0.5203359130577968), (0.59, 0.008395911556111807, 0.5236291783303145), (0.6, 0.0009908218606591572, 0.6421867281409518), (0.61, 0.0009386733416770963, 0.6481146056314836), (0.62, 0.0006779307467667918, 0.6584883912399144), (0.63, 0.00031289111389236547, 0.6739667380207476), (0.64, 0.00031289111389236547, 0.6767660135023876), (0.65, 0.00031289111389236547, 0.6826938909929194), (0.66, 0.00031289111389236547, 0.684834513420056), (0.67, 0.0, 0.7241890334266425), (0.68, 0.0, 0.7261649925901531), (0.69, 0.0, 0.7311048904989297), (0.7, 0.0, 0.7404906965256052), (0.71, 0.0, 0.7449366046435041), (0.72, 0.0, 0.74790054338877), (0.73, 0.0, 0.7485591964432735), (0.74, 0.0, 0.7587683187880784), (0.75, 0.0, 0.7610736044788408), (0.76, 0.0, 0.7833031450683352), (0.77, 0.0, 0.7862670838136012), (0.78, 0.0, 0.7889016960316153), (0.79, 0.0, 0.8030627367034414), (0.8, 0.0, 0.806685328503211), (0.81, 0.0, 0.8096492672484769), (0.82, 0.0, 0.8099785937757287), (0.83, 0.0, 0.8317141445743454), (0.84, 0.0, 0.8317141445743454), (0.85, 0.0, 0.8322081343652231), (0.86, 0.0, 0.8430759097645315), (0.87, 0.0, 0.843734562819035), (0.88, 0.0, 0.843734562819035), (0.89, 0.0, 0.843734562819035), (0.9, 0.0, 1.0), (0.91, 0.0, 1.0), (0.92, 0.0, 1.0), (0.93, 0.0, 1.0), (0.94, 0.0, 1.0), (0.95, 0.0, 1.0), (0.96, 0.0, 1.0), (0.97, 0.0, 1.0), (0.98, 0.0, 1.0), (0.99, 0.0, 1.0), (1.0, 0.0, 1.0)]
# Unweighted Boosted Tree Test Rate [(0.0, 1.0, 0.0), (0.01, 1.0, 0.0), (0.02, 1.0, 0.0), (0.03, 1.0, 0.0), (0.04, 1.0, 0.0), (0.05, 1.0, 0.0), (0.06, 1.0, 0.0), (0.07, 1.0, 0.0), (0.08, 1.0, 0.0), (0.09, 1.0, 0.0), (0.1, 1.0, 0.0), (0.11, 0.7602019352124527, 0.0), (0.12, 0.7602019352124527, 0.0), (0.13, 0.7602019352124527, 0.0), (0.14, 0.7219183845183004, 0.0012836970474967907), (0.15, 0.6970971813209929, 0.006418485237483954), (0.16, 0.6436684896928901, 0.007702182284980745), (0.17, 0.636095919225915, 0.007702182284980745), (0.18, 0.5830879259570888, 0.012836970474967908), (0.19, 0.5511148506520824, 0.014120667522464698), (0.2, 0.5439629785443837, 0.014120667522464698), (0.21, 0.5191417753470762, 0.01797175866495507), (0.22, 0.5082036180058898, 0.019255455712451863), (0.23, 0.48464450988641145, 0.02053915275994865), (0.24, 0.4795961295750947, 0.021822849807445442), (0.25, 0.45225073622212875, 0.028241335044929396), (0.26, 0.442995372318048, 0.029525032092426188), (0.27, 0.41312578880942363, 0.037227214377406934), (0.28, 0.4030290281867901, 0.03979460847240052), (0.29, 0.39671855279764406, 0.0410783055198973), (0.3, 0.37904922170803534, 0.04749679075738126), (0.31, 0.3710559528817838, 0.055198973042362), (0.32, 0.34791754312158185, 0.06418485237483953), (0.33, 0.33782078249894826, 0.06546854942233633), (0.34, 0.30963399242742956, 0.07445442875481387), (0.35, 0.297433740008414, 0.07830551989730423), (0.36, 0.2860748843079512, 0.0834403080872914), (0.37, 0.2797644089188052, 0.08472400513478819), (0.38, 0.2684055532183425, 0.09884467265725289), (0.39, 0.2532604122843921, 0.10397946084724005), (0.4, 0.2490534286916281, 0.10783055198973042), (0.41, 0.22801851072780815, 0.1245186136071887), (0.42, 0.2221287336979386, 0.12965340179717585), (0.43, 0.1737484223811527, 0.17201540436456997), (0.44, 0.1708035338662179, 0.17329910141206675), (0.45, 0.16954143878838873, 0.17586649550706032), (0.46, 0.12620950778291964, 0.25288831835686776), (0.47, 0.11064366848969288, 0.29781771501925547), (0.48, 0.09928481278923013, 0.3157894736842105), (0.49, 0.09718132099284812, 0.32605905006418484), (0.5, 0.08329827513672697, 0.3478818998716303), (0.51, 0.05511148506520824, 0.4428754813863928), (0.52, 0.05174589819099706, 0.4492939666238768), (0.53, 0.04501472444257468, 0.4813863928112965), (0.54, 0.03954564577198149, 0.5109114249037228), (0.55, 0.033655868742111905, 0.5507060333761232), (0.56, 0.03197307530500631, 0.5789473684210527), (0.57, 0.031131678586453514, 0.5815147625160462), (0.58, 0.02818679007151872, 0.6110397946084724), (0.59, 0.026924694993689526, 0.6123234916559692), (0.6, 0.011358855700462769, 0.6906290115532734), (0.61, 0.010096760622633571, 0.6919127086007703), (0.62, 0.008413967185527976, 0.6957637997432606), (0.63, 0.005469078670593185, 0.7060333761232349), (0.64, 0.005469078670593185, 0.7098844672657253), (0.65, 0.005048380311316786, 0.711168164313222), (0.66, 0.005048380311316786, 0.711168164313222), (0.67, 0.0033655868742111907, 0.7368421052631579), (0.68, 0.002944888514934792, 0.7368421052631579), (0.69, 0.002103491796381994, 0.7432605905006419), (0.7, 0.0016827934371055953, 0.7496790757381258), (0.71, 0.0016827934371055953, 0.754813863928113), (0.72, 0.0012620950778291964, 0.7573812580231065), (0.73, 0.0012620950778291964, 0.7573812580231065), (0.74, 0.00042069835927639884, 0.766367137355584), (0.75, 0.00042069835927639884, 0.766367137355584), (0.76, 0.00042069835927639884, 0.7907573812580231), (0.77, 0.00042069835927639884, 0.7933247753530167), (0.78, 0.00042069835927639884, 0.7958921694480102), (0.79, 0.0, 0.8023106546854942), (0.8, 0.0, 0.8125802310654685), (0.81, 0.0, 0.8138639281129654), (0.82, 0.0, 0.8138639281129654), (0.83, 0.0, 0.8433889602053916), (0.84, 0.0, 0.8433889602053916), (0.85, 0.0, 0.8433889602053916), (0.86, 0.0, 0.8510911424903723), (0.87, 0.0, 0.8510911424903723), (0.88, 0.0, 0.8510911424903723), (0.89, 0.0, 0.8510911424903723), (0.9, 0.0, 1.0), (0.91, 0.0, 1.0), (0.92, 0.0, 1.0), (0.93, 0.0, 1.0), (0.94, 0.0, 1.0), (0.95, 0.0, 1.0), (0.96, 0.0, 1.0), (0.97, 0.0, 1.0), (0.98, 0.0, 1.0), (0.99, 0.0, 1.0), (1.0, 0.0, 1.0)]




